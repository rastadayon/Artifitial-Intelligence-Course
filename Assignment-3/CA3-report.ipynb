{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA#3 of AI Course\n",
    "Rasta Tadayon\n",
    "810196436 \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Classification of news dataset with naive bayes </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Breif Description of the Project</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course project the objective was predicting category of a given news article given its headline and a short description of it. To do so **naive bayes** was used. **Naive Bayes** classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with assumption of strong independences between the features. \n",
    "\n",
    "The solution used here benefits from **Bag of words** method. In this method each word in the news description is viewed as a feature and number of its repetitions in each class is calculated. Eventually for each word the probability of it belonging to a certain class is calculated. Then for a new set of news articles the words and probability of them belonging to each class is calculated using Bayes' theorem and comparing them to detect the category of the new article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Project Steps</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset given for training and test have <code>authors</code>, <code>category</code>, <code>date</code>, <code>headline</code>, <code>link</code> and a <code>short_description</code> columns. The <code>short_description</code> and <code>headline</code> columns are used for the prediction. In order to be able to use the sentences or words given in these columns, they should be preprocessed.\n",
    "    \n",
    "After the preprocessing the dataset is divided into two datasets: training set, evaluation set. Then if the training set is not balanced it will be balanced out using **oversampling method** which will be explained later.\n",
    "\n",
    "After oversampling is done the dataset given is divided into two datasets, one is the training set and the other is the evaluation set.\n",
    "\n",
    "Then for each category in the training set a **bag of words** is created which consists of the words that appeared in that category and the number of times those words were repeated.\n",
    "\n",
    "Using the **bag of words** and naive bayes the probability of the news article belonging to each category(posterior probabiliry) is calculated. The largest probability among the calculated probabilities is selected as the article's category.\n",
    "\n",
    "Then precision metrics are used to determine how good the model is working. If the model is of desired standard it will be applied to our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Attachment/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Preprocess</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing phase a new column is creted which is the result of concatination of the <code>headline</code> and <code>short_description</code> columns and the following happens to the created column (<code>description_and_headline</code> column):\n",
    "- It is made lower case.\n",
    "- Stop words which are words like \"the\", \"that\",.. which are commenly used in English language ,and words with lengths under 2 are deleted from the text.\n",
    "- Text is tokenized by non alphabetic characters as delimiters.\n",
    "- Words are normalized using Stemming or Lemmatization methods.\n",
    "\n",
    "In this project both of stemming and lemmatization were used to preprocess the data to compare and choose the best of them.\n",
    "\n",
    "**Stemming** is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the langauge. Stems are created by removing the suffixes or prefixes used with a word.\n",
    "\n",
    "**Lemmatization**, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma is the canonical form, dictionary form, or citation form of a set of words. Since lemmatization returns a valid word of the language, it is used where it is necessary to get valid words.\n",
    "\n",
    "In this project the <code>Preprocessor</code> class is used to normalize the text. This class has a <code>preprocess</code> method which applies all the steps above. Firstly a new column is create in the dataset which consists of concatination of both headline and short_description strings together. Then the normalization happens to this newly created column.\n",
    "After the preprocess is done two new datasets are created which one is normalized using *Stemming* method and the other using *Lemmatization* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self,dataset):\n",
    "        self.initdf = dataset\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.preprocess()\n",
    "    def normalizer(self,text):\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        remNumbers = re.sub('[0-9]+', '', text)\n",
    "        remURL=re.sub(r'http\\S+', '',remNumbers)\n",
    "        tokens = self.tokenizer.tokenize(remURL)  \n",
    "        filteredWords = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "        self.stemWords=[self.stemmer.stem(w) for w in filteredWords]\n",
    "        self.lemmaWords=[self.lemmatizer.lemmatize(w) for w in filteredWords] #something is weird here\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        self.normalizer(text)\n",
    "        return ' '.join(self.lemmaWords)\n",
    "    def stem(self,text):\n",
    "        self.normalizer(text)\n",
    "        return ' '.join(self.stemWords)\n",
    "    def preprocess(self):\n",
    "        self.processedDf = self.initdf.copy()\n",
    "        self.processedDf = self.processedDf.replace(np.nan, '', regex=True)\n",
    "\n",
    "        self.stemmedDf = self.processedDf.copy()\n",
    "        self.lemmatizedDf = self.processedDf.copy()\n",
    "        self.stemmedDf['description_and_headline'] = self.stemmedDf['headline'] + self.stemmedDf['short_description']\n",
    "        self.lemmatizedDf['description_and_headline'] = self.lemmatizedDf['headline'] + self.lemmatizedDf['short_description']\n",
    "        \n",
    "        self.stemmedDf['description_and_headline'] = self.stemmedDf['description_and_headline'].apply(lambda x: self.stem(x))\n",
    "        self.lemmatizedDf['description_and_headline'] = self.lemmatizedDf['description_and_headline'].apply(lambda x: self.lemmatize(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1Preprocessor = Preprocessor(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>description_and_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "      <td>eccentr bingo parti paella island hop oslopåsk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td></td>\n",
       "      <td>lawyer drive forc behind mortgag scam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "      <td>madonna truth dare shoe line debut fall photo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "      <td>sophist seren la vega stripbut someth coupl sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "      <td>still pretti hard women get free birth control...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  Påskekrim is merely the tip of the proverbial ...   \n",
       "1                                                      \n",
       "2  Madonna is slinking her way into footwear now,...   \n",
       "3  But what if you're a 30-something couple that ...   \n",
       "4  Obamacare was supposed to make birth control f...   \n",
       "\n",
       "                            description_and_headline  \n",
       "0  eccentr bingo parti paella island hop oslopåsk...  \n",
       "1              lawyer drive forc behind mortgag scam  \n",
       "2  madonna truth dare shoe line debut fall photo ...  \n",
       "3  sophist seren la vega stripbut someth coupl sh...  \n",
       "4  still pretti hard women get free birth control...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase1Preprocessor.stemmedDf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>description_and_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "      <td>eccentricity bingo party paella island hopping...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td></td>\n",
       "      <td>lawyer driving force behind mortgage scam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "      <td>madonna truth dare shoe line debut fall photo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "      <td>sophistication serenity la vega stripbut somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "      <td>still pretty hard woman get free birth control...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  Påskekrim is merely the tip of the proverbial ...   \n",
       "1                                                      \n",
       "2  Madonna is slinking her way into footwear now,...   \n",
       "3  But what if you're a 30-something couple that ...   \n",
       "4  Obamacare was supposed to make birth control f...   \n",
       "\n",
       "                            description_and_headline  \n",
       "0  eccentricity bingo party paella island hopping...  \n",
       "1          lawyer driving force behind mortgage scam  \n",
       "2  madonna truth dare shoe line debut fall photo ...  \n",
       "3  sophistication serenity la vega stripbut somet...  \n",
       "4  still pretty hard woman get free birth control...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase1Preprocessor.lemmatizedDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the the stemmed dataset has words which don't exist in the English language but all the words created in the lemmatized dataset are valid English words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Dividing the Dataset into Training and Evaluation Set</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset given is divided into two new datasets: training set and evaluation set. The reason for that is to use the training set to train out model and the evaluation set to evaluate how good the model works (using precision metrics). \n",
    "\n",
    "Usually 80 percent of the dataset is used as the training set and left 20 percent is used as evaluation set.\n",
    "\n",
    "The division of the dataset is done using the <code>DatasetDivider</code> class which makes two new datasets representing training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDivider():\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset.copy()\n",
    "        self.dfs = []\n",
    "        self.trainDf = pd.DataFrame()\n",
    "        self.testDf = pd.DataFrame()\n",
    "    def divide(self, percentage = 0.8):\n",
    "        differentLabels = list(self.dataset.category.unique())\n",
    "        for label in differentLabels:\n",
    "            df =  self.dataset[self.dataset['category'] == label].copy()\n",
    "            msk = np.random.rand(df.shape[0]) < 0.8\n",
    "            dfList = []\n",
    "            dfList.append(df[msk])\n",
    "            dfList.append(df[~msk])\n",
    "            self.dfs.append(dfList)\n",
    "        for df in self.dfs:\n",
    "            self.trainDf = pd.concat([self.trainDf,df[0]]).sort_index()\n",
    "            self.testDf = pd.concat([self.testDf, df[1]]).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dividerLemmatized = DatasetDivider(phase1Preprocessor.lemmatizedDf)\n",
    "dividerLemmatized.divide()\n",
    "dividerStemmed = DatasetDivider(phase1Preprocessor.stemmedDf)\n",
    "dividerStemmed.divide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizedTrainDf = dividerLemmatized.trainDf\n",
    "lemmatizedTestDf = dividerLemmatized.testDf\n",
    "stemmedTrainDf = dividerStemmed.trainDf\n",
    "stemmedTestDf = dividerStemmed.testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Oversampling</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to wikipedia oversampling data analysis in a technique used to adjust the distribution of a class in a data set meaning the ratio between the different categories present in the dataset. Oversampling is used both in statistical sampling, survey design methodology and in machine learning.\n",
    "\n",
    "For oversampling in this project the categories with less data are replicated to reach the amount of the largest category. \n",
    "\n",
    "Oversampling involves introducing a bias to select more samples from one class than from another, to compensate for an imbalance that is either already present in the data, or likely to develop if a purely random sample were taken.\n",
    "\n",
    "In this project the <code>Oversampler</code> class does the oversampling. How it is done is by replicating minor classes until the sizes between all classes are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oversampler():\n",
    "    def __init__(self,trainDf):\n",
    "        self.trainDf = trainDf\n",
    "    \n",
    "    def oversample(self):\n",
    "        labels = self.trainDf.category.unique()\n",
    "        categorySizes = []\n",
    "        for label in labels:\n",
    "            df = self.trainDf[self.trainDf['category'] == label]\n",
    "            categorySizes.append(df.shape[0])\n",
    "        maxCategorySize = max(categorySizes)\n",
    "        for label in labels:\n",
    "            df = self.trainDf[self.trainDf['category'] == label]\n",
    "            if(df.shape[0] < maxCategorySize):\n",
    "                tempdf = df.sample(maxCategorySize - df.shape[0])\n",
    "                self.trainDf = pd.concat([self.trainDf, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversamplerLemmatized = Oversampler(lemmatizedTrainDf)\n",
    "oversamplerLemmatized.oversample()\n",
    "oversamplerStemmed = Oversampler(stemmedTrainDf)\n",
    "oversamplerStemmed.oversample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7096"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(oversamplerLemmatized.trainDf['category'] == 'BUSINESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7096"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(oversamplerLemmatized.trainDf['category'] == 'TRAVEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7096"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(oversamplerLemmatized.trainDf['category'] == 'STYLE & BEAUTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is observable the number of data in every category is equalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Bayesian Rule</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the steps mentioned before are done the probability of each article belonging to each category should be calculated and the largest probability will be selected as the estimated category.\n",
    "To calculate the probability **Bayes theorem** is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P(c|X) & = \\frac{P(X|c)P(c)}{P(X)} \\\\\n",
    "P(c|X) & = P(x_1|c)P(x_2|c)P(x_3|c)...P(x_n|c)P(c)\n",
    "\\end{align}\n",
    "\n",
    "- Posterior Probabality ($P(c|X)$): The probabality of an article belonging to class  with the condition of conrtainin words X in its description data.\n",
    "- Likelihood ($P(X|c)$): This probabality indicates the liklihood of the feature belonging to class c. Likelihood is the product of number of repetition of all words divided by total number of words in class c.\n",
    "- Class Prior Probabality ($P(c)$): The probabality of a news being type c. This probability is equal to the number of documents that belong to the class c devided by the number of all documents..\n",
    "- Predictor Prior Probabality ($P(X)$): The probabality of occurance of word $x_i$ in the news. Since for deciding the label of the data the calculated $P(c_i|x)$ probabilities are compared, the probability of $P(x)$ is negligible.\n",
    "\n",
    "As stated before in **Naive Bayes** the featured are assumed to be independent therefore the probability of a document belonging to a class is the product of probabilty of each word in the document belonging to that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Phase 1</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is done in two phases. In the first phase predictions will only happen between the two categories of 'BUSINESS' and 'TRAVEL' and the second phase predictions will be done between all three categories('STYLE & BEAUTY', 'BUSINESS' and 'TRAVEL')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class <code>BayesianClassifier</code> does the classification job. It inputs the training data and accordingly creates *bag of words* for each class (or category) in the class using the 'description_and_headline' column created in the previous steps. After creating the bag of words, the <code>estimate</code> function which inputs the test set, estimates the category for each article. To do so it firstly calculates the posterior probability of each article which indicates the probability of that article belonging to that specific class and is calculated like the following:\n",
    "\n",
    "\\begin{align}\n",
    "P(c|X) & = P(x_1|c)P(x_2|c)P(x_3|c)...P(x_n|c)P(c)\n",
    "\\end{align}\n",
    "$P(x_i|c_j)$ probability is calculated by the count that the word occurred in documents of class j, divided by the sum of the counts of each word in our vocabulary in class j.\n",
    "This posterior probability is calculated for each class $c_j$ and the largest probability indicates the class it belongs to.\n",
    "\n",
    "Since the probabilities might become so small that the python interpreter might round it to zero, for calculating the probability the logarithm of the probabilities are summed up which is equivalent to logarithm of product of all $P(x_i|c_j)$. Because logarithm is an incrementing function when used for comparing it does not effect the answer.\n",
    "\n",
    "It is notable to say when the word $x_i$ does not exist in the bag of words for class $c_j$ the probability is not considered zero, for it will make the posterior probability equal to zero which is not correct, rather it is considered a very small number such as $10^{-8}$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBagOfWords(trainDf):\n",
    "    \n",
    "    labels = trainDf.category.unique()\n",
    "    vocabCount = {}\n",
    "    bagOfWords = []\n",
    "    \n",
    "    for label in labels:\n",
    "        df = trainDf[trainDf['category'] == label]\n",
    "\n",
    "        description = list(df['description_and_headline']) \n",
    "        vocab = []\n",
    "        for item in description:\n",
    "            vocab += item.split()\n",
    "        bagOfWord = dict()\n",
    "        for word in vocab:\n",
    "            bagOfWord[word] = vocab.count(word)\n",
    "        bagOfWords.append([label, bagOfWord])\n",
    "        vocabCount[label] = sum(bagOfWord.values())\n",
    "    \n",
    "    return vocabCount, bagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianClassifier():\n",
    "    def __init__(self, trainDf):\n",
    "        self.trainDf = trainDf.copy()\n",
    "        self.bagOfWords = []\n",
    "        self.P_c = {}\n",
    "        self.vocabCount = {}\n",
    "        labels = self.trainDf.category.unique()\n",
    "        for label in labels:\n",
    "            df = self.trainDf[self.trainDf['category'] == label]\n",
    "            self.P_c[label] = df.shape[0]/self.trainDf.shape[0]\n",
    "    def createBagOfWords(self, vocabCount, bagOfWords):\n",
    "        self.bagOfWords = bagOfWords\n",
    "        self.vocabCount = vocabCount\n",
    "    def label(self, document):\n",
    "        doc = document.split()\n",
    "        probabilities = {}\n",
    "        for label in self.bagOfWords:\n",
    "            labelName = label[0]\n",
    "            p = math.log10(self.P_c[labelName])\n",
    "            for word in doc:\n",
    "                if word in label[1]:\n",
    "                    p += math.log10(label[1][word]/self.vocabCount[labelName])\n",
    "                else:\n",
    "                    p += -8\n",
    "            probabilities[labelName] = p\n",
    "        return max(probabilities.items(), key=operator.itemgetter(1))[0]\n",
    "    def estimate(self, testDf):\n",
    "        self.testDf = testDf.copy()\n",
    "        self.testDf['estimated_category'] = self.testDf['description_and_headline'].apply(lambda x: self.label(x))\n",
    "        \n",
    "    def accuracy(self):\n",
    "        return sum(self.testDf['category'] == self.testDf['estimated_category'])/self.testDf.shape[0]\n",
    "    \n",
    "    def precision(self, label):\n",
    "        TP = sum((self.testDf['estimated_category'] == label) & (self.testDf['category'] == label))\n",
    "        TP_plus_FP = sum(list(self.testDf['estimated_category'] == label))\n",
    "        return TP/TP_plus_FP\n",
    "    \n",
    "    def recall(self, label):\n",
    "        TP = sum((self.testDf['estimated_category'] == label) & (self.testDf['category'] == label))\n",
    "        FN = sum((self.testDf['estimated_category'] != label) & (self.testDf['category'] == label))\n",
    "        return TP/(TP+FN)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        labels = list(self.trainDf.category.unique())\n",
    "        if(len(labels) == 2):\n",
    "            columnNames = ['Phase 1']\n",
    "        elif(len(labels) == 3):\n",
    "            columnNames = ['Phase 2']\n",
    "        columnNames += labels\n",
    "        \n",
    "        outDf = pd.DataFrame(columns=columnNames)\n",
    "        outDf[columnNames[0]] = ['Recall', 'Precision', 'Accuracy']\n",
    "        for label in columnNames[1:]:\n",
    "            outDf[label] = [self.recall(label), self.precision(label), self.accuracy()]\n",
    "        \n",
    "        return outDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1DfLemmatized = oversamplerLemmatized.trainDf[oversamplerLemmatized.trainDf['category'] != 'STYLE & BEAUTY']\n",
    "phase1DfStemmed = oversamplerStemmed.trainDf[oversamplerStemmed.trainDf['category'] != 'STYLE & BEAUTY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1testDfLemmatized = lemmatizedTestDf[lemmatizedTestDf['category'] != 'STYLE & BEAUTY']\n",
    "phase1testDfStemmed = stemmedTestDf[stemmedTestDf['category'] != 'STYLE & BEAUTY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1Lemmatized = BayesianClassifier(phase1DfLemmatized)\n",
    "classifier1Stemmed = BayesianClassifier(phase1DfStemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabCountLemmitized, bagOfWordsLemmitized = createBagOfWords(phase1DfLemmatized)\n",
    "vocabCountStemmed, bagOfWordsStemmed = createBagOfWords(phase1DfStemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1Lemmatized.createBagOfWords(vocabCountLemmitized, bagOfWordsLemmitized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1Stemmed.createBagOfWords(vocabCountStemmed, bagOfWordsStemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1Lemmatized.estimate(phase1testDfLemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1Stemmed.estimate(phase1testDfStemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1</th>\n",
       "      <th>TRAVEL</th>\n",
       "      <th>BUSINESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.955050</td>\n",
       "      <td>0.860921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.923779</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.921006</td>\n",
       "      <td>0.921006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Phase 1    TRAVEL  BUSINESS\n",
       "0     Recall  0.955050  0.860921\n",
       "1  Precision  0.923779  0.915625\n",
       "2   Accuracy  0.921006  0.921006"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1Lemmatized.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1</th>\n",
       "      <th>BUSINESS</th>\n",
       "      <th>TRAVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.854824</td>\n",
       "      <td>0.954030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.921283</td>\n",
       "      <td>0.912595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.915709</td>\n",
       "      <td>0.915709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Phase 1  BUSINESS    TRAVEL\n",
       "0     Recall  0.854824  0.954030\n",
       "1  Precision  0.921283  0.912595\n",
       "2   Accuracy  0.915709  0.915709"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1Stemmed.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observsble that the two methods do not have that significant of a difference however the lemmatization method works slightly better and the reason is that the Stemming method only deletes the prefix and suffix of the word given but the Lemmatization method changes the word to its root form. On the downside Lemmatization method might take longer since it has an iterative approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Phase 2 - Classification Between All Three Classes</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase the classification happens on the whole dataset and between all three classes. The steps for this phase are just like the steps for the previous phase. Since we observed that the Lemmatization method worked better we aregoing to use the dataset which was normalized using this method from here on out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2Lemmatized = BayesianClassifier(oversamplerLemmatized.trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2VocabCount, phase2BagOfWords = createBagOfWords(oversamplerLemmatized.trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2Lemmatized.createBagOfWords(phase2VocabCount, phase2BagOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2Lemmatized.estimate(lemmatizedTestDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 2</th>\n",
       "      <th>TRAVEL</th>\n",
       "      <th>BUSINESS</th>\n",
       "      <th>STYLE &amp; BEAUTY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.925638</td>\n",
       "      <td>0.838394</td>\n",
       "      <td>0.915264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.886762</td>\n",
       "      <td>0.870804</td>\n",
       "      <td>0.937574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.902112</td>\n",
       "      <td>0.902112</td>\n",
       "      <td>0.902112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Phase 2    TRAVEL  BUSINESS  STYLE & BEAUTY\n",
       "0     Recall  0.925638  0.838394        0.915264\n",
       "1  Precision  0.886762  0.870804        0.937574\n",
       "2   Accuracy  0.902112  0.902112        0.902112"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2Lemmatized.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Confusion Matrix</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [geeksforgeeks website](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)  confusion matrix is a table that is used for describing the performance of a classifier on a set of test data for which the true values are known. It helps visualize the performance of a model.\n",
    "Most performance measures are computed from the confusion matrix for example accuracy, recall and precision.\n",
    "The confusion matrix for the phase 2 is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(df):\n",
    "    correctTravel = sum((df['category'] == 'TRAVEL') & (df['estimated_category'] == 'TRAVEL'))\n",
    "    businessPredictedTravel = sum((df['category'] == 'BUSINESS') & (df['estimated_category'] == 'TRAVEL'))\n",
    "    beautyPredictedTravel = sum((df['category'] == 'STYLE & BEAUTY') & (df['estimated_category'] == 'TRAVEL'))\n",
    "    \n",
    "    correctBusiness = sum((df['category'] == 'BUSINESS') & (df['estimated_category'] == 'BUSINESS'))\n",
    "    travelPredictedBusiness = sum((df['category'] == 'TRAVEL') & (df['estimated_category'] == 'BUSINESS'))\n",
    "    beautyPredictedBusiness = sum((df['category'] == 'STYLE & BEAUTY') & (df['estimated_category'] == 'BUSINESS'))\n",
    "    \n",
    "    travelPredictedBeauty = sum((df['category'] == 'TRAVEL') & (df['estimated_category'] == 'STYLE & BEAUTY'))\n",
    "    businessPredictedBeauty = sum((df['category'] == 'BUSINESS') & (df['estimated_category'] == 'STYLE & BEAUTY'))\n",
    "    correctBeauty = sum((df['category'] == 'STYLE & BEAUTY') & (df['estimated_category'] == 'STYLE & BEAUTY'))\n",
    "    \n",
    "    outDf = pd.DataFrame(columns=['-', 'Predicted: Travel', 'Predicted: Business', 'Predicted: Style & Beauty'])\n",
    "    outDf['-'] = ['Actual: Travel', 'Actual: Business', 'Actual: Style & Beauty']\n",
    "    outDf['Predicted: Travel'] = [correctTravel, businessPredictedTravel, beautyPredictedTravel]\n",
    "    outDf['Predicted: Business'] = [travelPredictedBusiness, correctBusiness, beautyPredictedBusiness]\n",
    "    outDf['Predicted: Style & Beauty'] = [travelPredictedBeauty, businessPredictedBeauty, correctBeauty]\n",
    "    return outDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-</th>\n",
       "      <th>Predicted: Travel</th>\n",
       "      <th>Predicted: Business</th>\n",
       "      <th>Predicted: Style &amp; Beauty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actual: Travel</td>\n",
       "      <td>1668</td>\n",
       "      <td>77</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actual: Business</td>\n",
       "      <td>117</td>\n",
       "      <td>856</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actual: Style &amp; Beauty</td>\n",
       "      <td>96</td>\n",
       "      <td>50</td>\n",
       "      <td>1577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        -  Predicted: Travel  Predicted: Business  \\\n",
       "0          Actual: Travel               1668                   77   \n",
       "1        Actual: Business                117                  856   \n",
       "2  Actual: Style & Beauty                 96                   50   \n",
       "\n",
       "   Predicted: Style & Beauty  \n",
       "0                         57  \n",
       "1                         48  \n",
       "2                       1577  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionMatrix(classifier2Lemmatized.testDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Prediction on New News Articles</h2> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Attachment/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPreprocessor = Preprocessor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2Lemmatized.estimate(testPreprocessor.lemmatizedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = classifier2Lemmatized.testDf.copy()\n",
    "predicted = predicted.drop(columns = ['headline', 'authors', 'link', 'short_description', 'date','description_and_headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predicted.rename(columns={\"estimated_category\": \"category\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- Lemmitization or Stemming?**\n",
    "\n",
    "Differences between the two methods were mentioned before but are rebrought in this section:\n",
    "**Stemming** is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the langauge. Stems are created by removing the suffixes or prefixes used with a word.\n",
    "\n",
    "**Lemmatization**, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma is the canonical form, dictionary form, or citation form of a set of words. Since lemmatization returns a valid word of the language, it is used where it is necessary to get valid words.\n",
    "\n",
    "In this project the results using either of these methods did not differ much however Lemmatization method worked slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2- What is tf-idf and how would it be applied to this project?**\n",
    "\n",
    "According to wikipedia TFIDF (short for term frequency–inverse document frequency), is a numerical statistic that is intended to reflect how important a word is to a document in a collection. TF-IDF is calculated by multiplying two terms:\n",
    "- **TF**: The number of times a term occurs in a document is called its term frequency. \n",
    "- **IDF**: The inverse document frequency of the word across a set of documents.\n",
    "The tfidf equation is as follows:\n",
    "\n",
    "$ TFIDF(t,d,D)=tf(t,d).idf(t,D)$\n",
    "\n",
    "For the term frequency tf(t,d), the simplest choice is to use the raw count of a term in a document, i.e., the number of times that term t occurs in document d which is what was used in this project but there are more advanced strategies available as well.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides meaning how common or rare it is across all documents. It is obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient:\n",
    "\n",
    "$idf(t,D)=\\log{\\frac{N}{|\\{d\\in D:t\\in d\\}|}}$\n",
    "\n",
    "N is the total number of documents.\n",
    "D is the set of all documnets.\n",
    " $|\\{d \\in D: t \\in d\\}|$ is number of documents where the term t appears.\n",
    "\n",
    "The idf term as explained above could have been used to increase the precision of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3- Why a high precision alone is not a good performance metric?**\n",
    "\n",
    "Precision is a measurement of how close the measured values are to each other, therefore if the predicted values are close to each other the precision is high however these close predictions can be far from the actual value therefore have very low accuracy.\n",
    "\n",
    "For example in a spam detector that takes many features into account the precision could be **very** high but the actual model could not be detecting correctly therefore the accuracy can be low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4- What would happen if a word is only repeated once in one category?** \n",
    "\n",
    "Since how the code in this project works is by multiplyin a very small probability when a category does not have the word, in the category which actually has seen that word once, there is a big chance, that category is selected for the prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
